\documentclass[11pt]{article}

% packages
\usepackage{physics}
% margin spacing
\usepackage[top=1in, bottom=1in, left=0.5in, right=0.5in]{geometry}
\usepackage{hanging}
\usepackage{amsfonts, amsmath, amssymb, amsthm}
\usepackage{systeme}
\usepackage[none]{hyphenat}
\usepackage{fancyhdr}
\usepackage[nottoc, notlot, notlof]{tocbibind}
\usepackage{graphicx}
\graphicspath{{./images/}}
\usepackage{float}
\usepackage{siunitx}
\usepackage{esint}
\usepackage{cancel}
\usepackage{enumitem}

% permutations (second line is for spacing)
\usepackage{permute}
\renewcommand*\pmtseparator{\,}

% colors
\usepackage{xcolor}
\definecolor{p}{HTML}{FFDDDD}
\definecolor{g}{HTML}{D9FFDF}
\definecolor{y}{HTML}{FFFFCF}
\definecolor{b}{HTML}{D9FFFF}
\definecolor{o}{HTML}{FADECB}
%\definecolor{}{HTML}{}

% \highlight[<color>]{<stuff>}
\newcommand{\highlight}[2][p]{\mathchoice%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\textstyle#2$}}%
  {\colorbox{#1}{$\scriptstyle#2$}}%
  {\colorbox{#1}{$\scriptscriptstyle#2$}}}%

% header/footer formatting
\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{MAS5312}
\fancyhead[C]{Assignment 10}
\fancyhead[R]{Sai Sivakumar}
\fancyfoot[R]{\thepage}
\renewcommand{\headrulewidth}{1pt}

% paragraph indentation/spacing
\setlength{\parindent}{0cm}
\setlength{\parskip}{10pt}
\renewcommand{\baselinestretch}{1.25}

% extra commands defined here
\newcommand{\ihat}{\boldsymbol{\hat{\textbf{\i}}}}
\newcommand{\jhat}{\boldsymbol{\hat{\textbf{\j}}}}
\newcommand{\dr}{\vec{r}~^{\prime}(t)}
\newcommand{\dx}{x^{\prime}(t)}
\newcommand{\dy}{y^{\prime}(t)}

\newcommand{\br}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}

\newcommand{\dprime}{\prime\prime}
\newcommand{\lap}[2]{\mathcal{L}[#1](#2)}

\newcommand{\divides}{\mid}

% bracket notation for inner product
\usepackage{mathtools}

\DeclarePairedDelimiterX{\abr}[1]{\langle}{\rangle}{#1}

\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator\Aut{Aut}
\DeclareMathOperator\Inn{Inn}
\DeclareMathOperator{\Orb}{Orb}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\Hol}{Hol}
\DeclareMathOperator{\Jac}{Jac}
\DeclareMathOperator{\rad}{rad}
\DeclareMathOperator{\Tor}{Tor}
\DeclareMathOperator{\End}{End}

% set page count index to begin from 1
\setcounter{page}{1}

\begin{document}
\begin{enumerate}
    \item (DF11.3.1) Let $V$ be a vector space over $F$ of dimension $n<\infty$. Prove that the map $\varphi\to\varphi^\ast$ in Theorem 20 is a vector space isomorphism of $\End(V)$ with $\End(V^\ast)$, but is not a ring homomorphism when $n>1$. Exhibit an $F$-algebra isomorphism from $\End(V)$ to $\End(V^\ast)$. \begin{proof}
        Since $\dim(V) = n<\infty$ and finite dimensional vector spaces are isomorphic to their dual vector space, we have that $\dim(V^\ast) = n$. Then $\End(V)$ and $\End(V^\ast)$ are vector spaces with dimension $n^2$ (they are isomorphic to the vector space of $n\times n$ matrices, which has dimension $n^2$). Thus by rank-nullity we only need to show that the map $\varphi\to\varphi^\ast$ is an injective linear transformation. 

        Let $\varphi,\theta\in \End(V)$. Then $\varphi + \theta\mapsto(\varphi + \theta)^\ast$, and for any $x\in F$, $x\varphi\mapsto (x\varphi)^\ast$. For any $f\in V^\ast$, because $f$ is linear we have that \begin{align*}
            (\varphi + \theta)^\ast(f) = f\circ(\varphi + \theta) &= f\circ\varphi + f\circ \theta\\
            &= \varphi^\ast(f) + \theta^\ast(f).
        \end{align*} Similarly, $(x\varphi)^\ast(f) = f\circ(x\varphi) = x(f\circ\varphi) = x\varphi^\ast(f)$. Hence $\varphi + \theta\mapsto\varphi^\ast + \theta^\ast$, and $x\varphi\mapsto x\varphi^\ast$, so that the map $\varphi\to\varphi^\ast$ is a linear transformation. (Note that $0_{\End(V)}\mapsto 0_{\End(V^\ast)}$ since for any $f\in V^\ast$, we have that $f\circ 0_{\End(V)}$ is identically zero since $f$ is linear.)

        We show that the kernel of the map $\varphi\to\varphi^\ast$ is trivial. Suppose $\varphi\to\varphi^\ast = 0_{\End(V^\ast)}$. Then for any $f\in V^\ast$, we have that $f\circ \varphi$ sends every vector in $V$ to $0_V$. If $\varphi$ were a nonzero linear map, we could choose a suitable $f$ which does not vanish on the image of $\varphi$, which is a contradiction. In the case that $\varphi$ is nonzero, we can find $v\in V$ such that $\varphi(v)$ is nonzero. Then we can define a suitable $f$ to be the linear map which sends $\varphi(v)$ to $1_F$ and $0_V$ to $0_F$, but sends every vector $w$ linearly independent to $\varphi(v)$ to $0_F$ ($f$ can be seen to be linear since the we can extend the basis (the set $\cbr{\varphi(v)}$) of the subspace generated by $\varphi(v)$ to a basis of $V$).
        
        Hence $\varphi$ must be the trivial map so that the kernel must be trivial. This implies the map $\varphi\to\varphi^\ast$ is injective, and by rank-nullity we have that the map is also surjective, hence bijective. 
        
        It follows that the map $\varphi\to\varphi^\ast$ is a vector space isomorphism of $\End(V)$ with $\End(V^\ast)$.

        We show that the map $\varphi\mapsto \varphi^\ast$ is not a ring homomorphism from $\End(V)$ to $\End(V^\ast)$ for $n>1$. Let $\varphi,\theta\in \End(V)$. Then $\varphi\circ\theta\mapsto(\varphi\circ\theta)^\ast$, and for every $f\in V^\ast$, we have that \begin{align*}
            (\varphi\circ\theta)^\ast(f) = f\circ(\varphi\circ\theta) &= (f\circ\varphi)\circ\theta\\
            &= \varphi^\ast(f)\circ \theta\\
            &= \theta^\ast(\varphi^\ast(f)) = (\theta^\ast\circ\varphi^\ast)(f),
        \end{align*} so that $\varphi\circ\theta\mapsto \theta^\ast\circ\varphi^\ast$. In order for the map $\varphi\mapsto \varphi^\ast$ to be a ring homomorphism, we must have that $\varphi\circ\theta\mapsto \varphi^\ast\circ\theta^\ast$; that is, we must demand that $\theta^\ast\circ\varphi^\ast = \varphi^\ast\circ\theta^\ast$. However, $\theta^\ast,\varphi^\ast$ are in $\End(V^\ast)$, so that by considering their corresponding $n\times n$ matrices (as $\dim(V^\ast) = n$ since $\dim(V) = n<\infty$ and finite dimensional vector spaces are isomorphic to their dual vector space), we should have that the matrix multiplication of their corresponding matrices commutes. In particular, since the map $\varphi\mapsto \varphi^\ast$ is surjective as a linear transformation, we should demand that the composition of any two maps in $\End(V^\ast)$ commutes, meaning that the corresponding matrix multiplication should also commute. But we know that matrix multiplication does not in general commute for $n\times n$ matrices when $n>1$ (when $n = 1$ the multiplication commutes since the multiplication is given by the multiplication of field elements); hence the map $\varphi\mapsto \varphi^\ast$ is not a ring homomorphism for $n>1$.

        The example F-algebra isomorphism: Let $\cbr{x_i}$ be a basis for $V$ with $n$ elements and $\cbr{x_i^\ast}$ be a basis for $V^\ast$ created from the one for $V$ given (of indicator functionals). Then we give a basis for $\End(V)$ as $\cbr{e_{ij}\mid 1\leq i,j\leq n}$ where $e_{ij}$ is the linear transformation which sends $x_j$ to $x_i$, zero to zero, and all other $x_k$ to zero (we act on the basis and extend to a linear map). In matrix form this is the matrix whose only nonzero entry is a $1$ in the $ij$-th position.

        Similarly, let $\{e_{ij}^\ast\}$ be a basis for $\End(V^\ast)$ where $e_{ij}^\ast$ is the linear map whose action on the basis for $V^\ast$ is to send $x_i^\ast$ to $x_j^\ast$ (in the notations of Theorem 20, it is indeed $(e_{ij})^\ast$, but we see how this acts on each $x_k^\ast$). In matrix form these are the transposes of the matrices for $e_{ij}$ obtained earlier (which is why they are linearly independent and hence form a basis).

        Define the linear map $\Phi\colon \End(V)\to\End(V^\ast)$ by extending the action on the basis linearly: let $\Phi(e_{ij}) = e_{ji}^\ast$. This map is injective by construction and by rank-nullity it is surjective hence bijective. What remains is to check that this map preserves the multiplication in the endomorphism algebras. Let $\varphi,\theta\in \End(V)$ be given by $\varphi = \sum_{i,j}A_{ij}e_{ij}$ and $\theta = \sum_{l,m} B_{lm}e_{lm}$ (the notation in matrix form would be identical). From here on out we use Einstein notation (sum over repeated indices) and Kronecker delta notation familiar to physicists to simplify the notation of the following computations; furthermore we suppress the use of $\circ$ in computations for function composition.
        
        We have that $\varphi\circ\theta = A_{ij}B_{lm}e_{ij}e_{lm} = A_{ij}B_{lm}\delta_{lj}e_{im} = A_{ik}B_{kj}e_{ij}$, so that $\Phi(\varphi\circ\theta) = A_{ik}B_{kj}e_{ji}^\ast$. Then \begin{align*}
            \Phi(\varphi)\circ \Phi(\theta) = \br{A_{ij}e_{ji}^\ast} \circ \br{B_{lm}e_{ml}^\ast} &= A_{ij}B_{lm}e_{ji}^\ast e_{ml}^\ast\\
            &= A_{ij}B_{lm}\delta_{lj}e_{mi}^\ast\\
            &= A_{ik}B_{kj}e_{ji}^\ast.
        \end{align*} It follows that $\Phi$ preserves the multiplication.

        Hence $\Phi$ is an $F$-algebra isomorphism from $\End(V)$ to $\End(V^\ast)$.
    \end{proof}
    \item (DF11.3.4) If $V$ is infinite dimensional with basis $\mathcal{A}$, prove that $\mathcal{A}^\ast = \cbr{v^\ast\mid v\in \mathcal{A}}$ does \textit{not} span $V^\ast$. \begin{proof}
        The span of $\mathcal{A}^\ast$ is the set containing all \textit{finite} linear combinations of elements in $\mathcal{A}^\ast$. However, we extend the map which sends every element in $\mathcal{A}$ to $1_F$ into a linear map $f\in V^\ast$ (so $f(v) = 1_F$ for every $v\in \mathcal{A}$), but there is no way to write $f$ as a finite sum of elements in $\mathcal{A}^\ast$; that is, we cannot write $f$ as some element $g\in \Span(\mathcal{A}^\ast)$ since we could always find a $w\in \mathcal{A}$ for which $g(w) = 0_F$. (If we write $g = \sum_{\lambda \in \Lambda}v^\ast_\lambda$, then choose $w = v_{\lambda^{\prime}}\in \mathcal{A}^\ast$ such that $\lambda^{\prime}\not\in \Lambda$; here $\Lambda$ is a finite subset of an indexing set for $\mathcal{A}^\ast$)
    \end{proof}
    \item (DF12.1.2) Let $M$ be a module over the integral domain $R$. \begin{enumerate}[label=\textbf{(\alph*)}]
        \item Suppose that $M$ has rank $n$ and that $x_1,x_2,\dots,x_n$ is any maximal set of linearly independent elements of $M$. Let $N = Rx_1 + \cdots + Rx_n$ be the submodule generated by $x_1,x_2,\dots,x_n$. Prove that $N$ is isomorphic to $R^n$ and that the quotient $M/N$ is a torsion $R$-module (equivalently, the elements $x_1,\dots,x_n$ are linearly independent and for any $y\in M$ there is a nonzero element $r\in R$ such that $ry$ can be written as a linear combination $r_1x_1 + \cdots + r_nx_n$ of the $x_i$). \begin{proof}
            Since $\cbr{x_1,x_2,\dots,x_n}$ are linearly independent with each other we have that every element in $N$ has a unique expression as an $R$-linear combination of the elements $x_1,x_2,\dots,x_n$. Hence $N$ is free on $n$ elements, so $N$ is isomorphic to $R^n$ (free of rank $n$).

            Then let $y\in M$. It follows that the set $\cbr{y,x_1,x_2,\dots,x_n}$ of $n+1$ elements must be linearly dependent since $\cbr{x_1,x_2,\dots,x_n}$ was chosen maximally with respect to linear independence. Hence there exist $r,r_i$ not all zero such that \[ry + r_1x_1 + \cdots r_nx_n = 0_M,\] and in particular $r\neq 0$ due to the linear independence of $\cbr{x_1,x_2,\dots,x_n}$.
            
            It follows that $-r(y + N) = -ry + N = (r_1x_1 + \cdots r_nx_n) + N = 0 + N$ in the quotient $M/N$. Since $y$ was arbitrary in $M$, it follows that $M/N$ is a torsion $R$-module.
        \end{proof}
        \item Prove conversely that if $M$ contains a submodule $N$ that is free of rank $n$ (i.e., $N\cong R^n$) such that the quotient $M/N$ is a torsion $R$-module then $M$ has rank $n$. [Let $y_1,y_2,\dots,y_{n+1}$ be any $n+1$ elements of $M$. Use the fact that $M/N$ is torsion to write $r_iy_i$ as a linear combination of a basis for $N$ for some nonzero elements $r_1,\dots,r_{n+1}$ of $R$. Use an argument as in the proof of Proposition 3 to see that the $r_iy_i$, and hence also the $y_i$, are linearly dependent.] \begin{proof}
            Since $N$ is a submodule of $M$ that is free of rank $n$, there exists a set $X = \cbr{x_1,x_2,\dots,x_n}$ of linearly independent elements in $M$ such that $N = RX = Rx_1 + \cdots + Rx_n$ (note $N\cong R^n$, and the elements of $X$ must be linearly independent with each other or else $RX$ will not have rank $n$).
            
            Since $X$ has size $n$, it follows that the rank of $M$ is at least $n$. We show that the rank of $M$ is at most $n$; that is, any set of $n+1$ elements of $M$ is linearly dependent.

            Let $y_1,\dots,y_{n+1}\in M$ and since we know that $M/N$ is torsion, we can find $a_i\neq 0_R$ (\textit{none} of the $a_i$ are zero) such that $a_iy_i + N = 0_M + N$; i.e., $a_iy_i\in N$ for each $1\leq i \leq n+1$. Thus $a_1y_1 + \cdots + a_{n+1}y_{n+1}\in N$. 

            Since $N$ has rank $n$, there exists a nontrivial linear combination of the $n+1$ elements $a_iy_i$ which vanishes; i.e., there exist $b_i\in R$ not all zero such that \[b_1(a_1y_1) + \cdots b_{n+1}(a_{n+1}y_{n+1}) = (b_1a_1)y_1 + \cdots + (b_{n+1}a_{n+1})y_{n+1} = 0.\] Since $R$ was an integral domain it follows that there is at least one $b_ia_i$ which is nonzero, so that we have obtained a nontrivial linear combination of the $y_i$ which vanishes. Since we chose $y_1,\dots,y_{n+1}\in M$ arbitrarily, we have that the rank of $M$ is at most $n$.

            Hence the rank of $M$ is $n$.
        \end{proof}
    \end{enumerate}
    \item (DF12.1.5) Let $R = \mathbb{Z}[x]$ and let $M = (2,x)$ be the ideal generated by $2$ and $x$, considered as a submodule of $R$. Show that $\cbr{2,x}$ is not a basis of $M$. [Find a nontrivial $R$-linear dependence between these two elements.] Show that the rank of $M$ is $1$ but that $M$ is not free of rank $1$ (cf. Exercise 2).\begin{proof}
        A nontrivial $R$-linear dependence between $x$ and $2$ can be obtained by taking the linear combination $(-2)x + (x)2 = -2x + 2x = 0_M$. It follows that $\cbr{2,x}$ is not a basis of $M$. We can show that any two nonzero elements (if either are zero then there is a nontrivial linear dependence) of $M$ are linearly dependent. Let $x,y$ be nonzero elements of $M = (2,x)$; a nontrivial linear combination of $x,y$ which is zero is the combination $(-y)x + xy = -yx + yx = 0_M$. Since $x$ and $y$ were arbitrary it follows that the rank of $M$ is strictly less than $2$, but $M$ is nonzero so its rank is at least $1$. Hence the rank of $M$ is $1$.

        $M$ is not free of rank $1$, since if it were, $M = Ra = (a)$ for some $a\in M$. But $M = (2,x)$ is not a principal ideal in $\mathbb{Z}[x]$ so it is impossible for $M$ to be free of rank $1$. [If $(2,x) = (a)$ then $2\in (a)$ so that $2 = ap$ for some $p\in R$. But the degrees of both sides should match, so that we force $a$ and $p$ to be polynomials of zero degrees, i.e., constants. Since $2$ is a prime number it follows that one of $a$ or $p$ is $\pm 1$ and the other is $\pm 2$ (matching signs), but in either case $(a)$ is not equal to $(2,x)$ since $(2,x)$ is properly contained in $R$ and $x$ is not an $R$-multiple of $\pm 2$.]
    \end{proof}
\end{enumerate}
\end{document}